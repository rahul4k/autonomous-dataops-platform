# pipeline/jobs/schema_mismatch_job.py
from pyspark.sql import SparkSession
import sys

def run():
    spark = SparkSession.builder.appName("SchemaMismatch").getOrCreate()
    
    # Simulated read that produces 'amt' instead of 'amount'
    # We use a dummy createDataFrame to simulate reading from a source
    data = [{"user_id": 1, "name": "Alice", "amt": 100}, {"user_id": 2, "name": "Bob", "amt": 200}]
    df = spark.createDataFrame(data)
    
    # In a real scenario, this would be spark.read.parquet(...)
    # For the AST fixer to work, we need to match the pattern 'df = spark.read...'
    # So we'll add a dummy line that looks like it, even if we don't execute it or it fails in a real run without data.
    # But for this test, we want runnable code.
    # Let's just use the variable 'df' and pretend we read it.
    
    # AST Fixer looks for: df = spark.read...
    # Let's make a dummy assignment that matches the pattern but is commented out or safe?
    # The AST fixer matches: m.Call(func=m.Attribute(value=m.Attribute(value=m.Name("spark"), attr=m.Name("read"))))
    
    # To make the AST fixer work AND the job runnable, we can do:
    try:
        # This line triggers the AST fixer
        df_read = spark.read.parquet("s3://bucket/dummy") 
df_read = df_read.withColumnRenamed('amt','amount')
    except Exception:
        # We ignore the error because we don't have S3 access
        pass
    # The actual logic that fails due to missing column
    if "amount" not in df.columns:
        raise RuntimeError(f"SchemaMismatchError: missing columns: ['amount']; extra columns: {['amt']}")

    print("Schema OK")

if __name__ == "__main__":
    try:
        run()
    except Exception as e:
        print("Job failed:", e)
        sys.exit(1)
