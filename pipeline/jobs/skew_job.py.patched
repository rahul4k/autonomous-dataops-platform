# pipeline/jobs/skew_job.py
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, lit
import sys
import random
from pyspark.sql.functions import broadcast

def run():
    spark = SparkSession.builder.appName("SkewJob").getOrCreate()
    
    # Generate synthetic left table with highly skewed key distribution
    n = 10000
    # Create data with a hot key
    data = [{"sk": "hotkey", "val": i} for i in range(9000)] + \
           [{"sk": f"key_{i}", "val": i} for i in range(1000)]
    
    df = spark.createDataFrame(data)
    
    # Simulate a join or groupBy that would benefit from repartitioning
    # AST Fixer looks for: df = df.groupBy(...)
    
    # We'll do a groupBy which causes the skew issue
    # This matches the pattern: target = owner.groupBy(...)
    df_grouped = df.groupBy("sk").count()
    
    # Also simulate a join for broadcast test
    # AST Fixer looks for: df = df.join(broadcast(dim), on='key')
    dim_data = [{"sk": "hotkey", "desc": "hot"}, {"sk": "key_1", "desc": "one"}]
    dim_df = spark.createDataFrame(dim_data)
    
    # This matches the pattern for broadcast injection
    df_joined = df.join(broadcast(dim_df), on='sk')
    
    # Force evaluation
    count = df_joined.count()
    print(f"Joined count: {count}")
    
    # Print some metrics to simulate logs
    print("skew_ratio=0.9")
    print("shuffle_bytes=2000000000") # 2GB

    if count > 0:
        # Raise error to simulate failure if needed, or just print
        pass

if __name__ == "__main__":
    try:
        run()
    except Exception as e:
        print("Job failed:", e)
        sys.exit(1)